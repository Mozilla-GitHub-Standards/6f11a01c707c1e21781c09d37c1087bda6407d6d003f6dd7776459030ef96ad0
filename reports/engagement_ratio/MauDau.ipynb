{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Firefox Engagement Ratio\n",
    "\n",
    "Compute the Engagement Ratio for the overall Firefox population as described in [Bug 1240849](https://bugzilla.mozilla.org/show_bug.cgi?id=1240849). The resulting data is shown on the [Firefox Dashboard](http://metrics.services.mozilla.com/firefox-dashboard/), and the more granular MAU and DAU values can be viewed via the [Diagnostic Data Viewer](https://metrics.services.mozilla.com/diagnostic-data-viewer).\n",
    "\n",
    "The actual Daily Active Users (DAU) and Monthly Active Users (MAU) computations are defined in [standards.py](https://github.com/mozilla/python_moztelemetry/blob/master/moztelemetry/standards.py) in the [python_moztelemetry](https://github.com/mozilla/python_moztelemetry) repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to parse whitelist (/home/hadoop/anaconda2/lib/python2.7/site-packages/moztelemetry/bucket-whitelist.json). Assuming all histograms are acceptable.\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 19.7 s\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime as _datetime, timedelta, date\n",
    "import boto3\n",
    "import botocore\n",
    "import csv\n",
    "import os.path\n",
    "from moztelemetry.standards import dau, mau\n",
    "\n",
    "bucket = \"telemetry-parquet\"\n",
    "prefix = \"executive_stream/v3\"\n",
    "%time dataset = sqlContext.read.load(\"s3://{}/{}\".format(bucket, prefix), \"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many cores are we running on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what do the underlying records look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- docType: string (nullable = false)\n",
      " |-- submissionDate: string (nullable = false)\n",
      " |-- activityTimestamp: double (nullable = false)\n",
      " |-- profileCreationTimestamp: double (nullable = false)\n",
      " |-- clientId: string (nullable = false)\n",
      " |-- documentId: string (nullable = false)\n",
      " |-- country: string (nullable = false)\n",
      " |-- channel: string (nullable = false)\n",
      " |-- os: string (nullable = false)\n",
      " |-- osVersion: string (nullable = false)\n",
      " |-- default: boolean (nullable = false)\n",
      " |-- buildId: string (nullable = false)\n",
      " |-- app: string (nullable = false)\n",
      " |-- version: string (nullable = false)\n",
      " |-- vendor: string (nullable = false)\n",
      " |-- reason: string (nullable = false)\n",
      " |-- hours: double (nullable = false)\n",
      " |-- google: integer (nullable = false)\n",
      " |-- yahoo: integer (nullable = false)\n",
      " |-- bing: integer (nullable = false)\n",
      " |-- other: integer (nullable = false)\n",
      " |-- pluginHangs: integer (nullable = false)\n",
      " |-- submission_date_s3: string (nullable = true)\n",
      " |-- channel_s3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to incrementally update the data, re-computing any values that are missing or for which data is still arriving. Define that logic here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fmt(the_date, date_format=\"%Y%m%d\"):\n",
    "    return _datetime.strftime(the_date, date_format)\n",
    "\n",
    "# Our calculations look for activity date reported within\n",
    "# a certain time window. If that window has passed, we do\n",
    "# not need to re-compute data for that period.\n",
    "def should_be_updated(record,\n",
    "        target_col=\"day\",\n",
    "        generated_col=\"generated_on\",\n",
    "        date_format=\"%Y%m%d\"):\n",
    "    target = _datetime.strptime(record[target_col], date_format)\n",
    "    generated = _datetime.strptime(record[generated_col], date_format)\n",
    "    \n",
    "    # Don't regenerate data that was already updated today.\n",
    "    today = fmt(_datetime.utcnow(), date_format)\n",
    "    if record[generated_col] >= today:\n",
    "        return False\n",
    "    \n",
    "    diff = generated - target\n",
    "    return diff.days <= 10\n",
    "\n",
    "# Identify all missing days, or days that have not yet passed\n",
    "# the \"still reporting in\" threshold (as of 2016-03-17, that is\n",
    "# defined as 10 days).\n",
    "def update_engagement_csv(dataset, old_filename, new_filename, \n",
    "                          cutoff_days=30, date_format=\"%Y%m%d\"):\n",
    "    cutoff_date = _datetime.utcnow() - timedelta(cutoff_days)\n",
    "    cutoff = fmt(cutoff_date, date_format)\n",
    "    print \"Cutoff date: {}\".format(cutoff)\n",
    "\n",
    "    fields = [\"day\", \"dau\", \"mau\", \"generated_on\"]\n",
    "\n",
    "    should_write_header = True\n",
    "    potential_updates = {}\n",
    "    # Carry over rows we won't touch\n",
    "    if os.path.exists(old_filename):\n",
    "        with open(old_filename) as csv_old:\n",
    "            reader = csv.DictReader(csv_old)\n",
    "            with open(new_filename, \"w\") as csv_new:\n",
    "                writer = csv.DictWriter(csv_new, fields)\n",
    "                writer.writeheader()\n",
    "                should_write_header = False\n",
    "                for row in reader:\n",
    "                    if row['day'] < cutoff:\n",
    "                        writer.writerow(row)\n",
    "                    else:\n",
    "                        potential_updates[row['day']] = row\n",
    "\n",
    "    with open(new_filename, \"a\") as csv_new:\n",
    "        writer = csv.DictWriter(csv_new, fields)\n",
    "        if should_write_header:\n",
    "            writer.writeheader()\n",
    "\n",
    "        for i in range(cutoff_days, 0, -1):\n",
    "            target_day = fmt(_datetime.utcnow() - timedelta(i), date_format)\n",
    "            if target_day in potential_updates and not should_be_updated(potential_updates[target_day]):\n",
    "                # It's fine as-is.\n",
    "                writer.writerow(potential_updates[target_day])\n",
    "            else:\n",
    "                # Update it.\n",
    "                print \"We should update data for {}\".format(target_day)\n",
    "                record = {\"day\": target_day, \"generated_on\": fmt(_datetime.utcnow(), date_format)}\n",
    "                print \"Starting dau {} at {}\".format(target_day, _datetime.utcnow())\n",
    "                record[\"dau\"] = dau(dataset, target_day)\n",
    "                print \"Finished dau {} at {}\".format(target_day, _datetime.utcnow())\n",
    "                print \"Starting mau {} at {}\".format(target_day, _datetime.utcnow())\n",
    "                record[\"mau\"] = mau(dataset, target_day)\n",
    "                print \"Finished mau {} at {}\".format(target_day, _datetime.utcnow())\n",
    "                writer.writerow(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch existing data from S3\n",
    "Attempt to fetch an existing data file from S3. If found, update it incrementally. Otherwise, re-compute the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutoff date: 20160216\n",
      "We should update data for 20160306\n",
      "Starting dau 20160306 at 2016-03-17 15:49:47.914425\n",
      "Finished dau 20160306 at 2016-03-17 15:51:45.302400\n",
      "Starting mau 20160306 at 2016-03-17 15:51:45.303002\n",
      "Finished mau 20160306 at 2016-03-17 16:01:26.936708\n",
      "We should update data for 20160307\n",
      "Starting dau 20160307 at 2016-03-17 16:01:26.937451\n",
      "Finished dau 20160307 at 2016-03-17 16:03:04.197129\n",
      "Starting mau 20160307 at 2016-03-17 16:03:04.197725\n",
      "Finished mau 20160307 at 2016-03-17 16:12:38.615409\n",
      "We should update data for 20160308\n",
      "Starting dau 20160308 at 2016-03-17 16:12:38.616142\n",
      "Finished dau 20160308 at 2016-03-17 16:16:15.368635\n",
      "Starting mau 20160308 at 2016-03-17 16:16:15.369213\n",
      "Finished mau 20160308 at 2016-03-17 16:24:32.296737\n",
      "We should update data for 20160309\n",
      "Starting dau 20160309 at 2016-03-17 16:24:32.297561\n",
      "Finished dau 20160309 at 2016-03-17 16:27:31.124053\n",
      "Starting mau 20160309 at 2016-03-17 16:27:31.124633\n",
      "Finished mau 20160309 at 2016-03-17 16:35:33.693763\n",
      "We should update data for 20160310\n",
      "Starting dau 20160310 at 2016-03-17 16:35:33.694491\n",
      "Finished dau 20160310 at 2016-03-17 16:38:38.172088\n",
      "Starting mau 20160310 at 2016-03-17 16:38:38.172649\n",
      "Finished mau 20160310 at 2016-03-17 16:47:20.224043\n",
      "We should update data for 20160311\n",
      "Starting dau 20160311 at 2016-03-17 16:47:20.224839\n",
      "Finished dau 20160311 at 2016-03-17 16:48:58.795029\n",
      "Starting mau 20160311 at 2016-03-17 16:48:58.795600\n",
      "Finished mau 20160311 at 2016-03-17 16:57:04.080876\n",
      "We should update data for 20160312\n",
      "Starting dau 20160312 at 2016-03-17 16:57:04.081609\n",
      "Finished dau 20160312 at 2016-03-17 16:57:52.876366\n",
      "Starting mau 20160312 at 2016-03-17 16:57:52.876954\n",
      "Finished mau 20160312 at 2016-03-17 17:05:17.862558\n",
      "We should update data for 20160313\n",
      "Starting dau 20160313 at 2016-03-17 17:05:17.863283\n",
      "Finished dau 20160313 at 2016-03-17 17:07:15.593273\n",
      "Starting mau 20160313 at 2016-03-17 17:07:15.593848\n",
      "Finished mau 20160313 at 2016-03-17 17:14:45.713349\n",
      "We should update data for 20160314\n",
      "Starting dau 20160314 at 2016-03-17 17:14:45.714060\n",
      "Finished dau 20160314 at 2016-03-17 17:15:26.090377\n",
      "Starting mau 20160314 at 2016-03-17 17:15:26.090960\n",
      "Finished mau 20160314 at 2016-03-17 17:23:01.374672\n",
      "We should update data for 20160315\n",
      "Starting dau 20160315 at 2016-03-17 17:23:01.375409\n",
      "Finished dau 20160315 at 2016-03-17 17:24:07.330859\n",
      "Starting mau 20160315 at 2016-03-17 17:24:07.331535\n",
      "Finished mau 20160315 at 2016-03-17 17:31:34.658254\n",
      "We should update data for 20160316\n",
      "Starting dau 20160316 at 2016-03-17 17:31:34.658929\n",
      "Finished dau 20160316 at 2016-03-17 17:31:47.014407\n",
      "Starting mau 20160316 at 2016-03-17 17:31:47.014995\n",
      "Finished mau 20160316 at 2016-03-17 17:38:45.866055\n"
     ]
    }
   ],
   "source": [
    "from boto3.s3.transfer import S3Transfer\n",
    "data_bucket = \"net-mozaws-prod-us-west-2-pipeline-analysis\"\n",
    "engagement_basename = \"engagement_ratio.csv\"\n",
    "new_engagement_basename = \"engagement_ratio.{}.csv\".format(_datetime.strftime(_datetime.utcnow(), \"%Y%m%d\"))\n",
    "s3path = \"mreid/maudau\"\n",
    "engagement_key = \"{}/{}\".format(s3path, engagement_basename)\n",
    "\n",
    "client = boto3.client('s3', 'us-west-2')\n",
    "transfer = S3Transfer(client)\n",
    "\n",
    "try:\n",
    "    transfer.download_file(data_bucket, engagement_key, engagement_basename)\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    # If the file wasn't there, that's ok. Otherwise, abort!\n",
    "    if e.response['Error']['Code'] != \"404\":\n",
    "        raise e\n",
    "    else:\n",
    "        print \"Did not find an existing file at '{}'\".format(engagement_key)\n",
    "\n",
    "update_engagement_csv(dataset, engagement_basename, new_engagement_basename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update data on S3\n",
    "Now we have an updated dataset on the local filesystem.\n",
    "\n",
    "Since it is so tiny, we keep a date-stamped backup of each dataset in addition to the \"latest\" file.\n",
    "\n",
    "Upload the updated file back to S3, as well as relaying it to the S3 bucket that automatically relays to the dashboard server. This final upload appears in the [Firefox Dashboard data dir](http://metrics.services.mozilla.com/firefox-dashboard/data/) as [engagement_ratio.csv](http://metrics.services.mozilla.com/firefox-dashboard/data/engagement_ratio.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Upload the updated csv file to S3\n",
    "\n",
    "# Update the day-specific file:\n",
    "new_s3_name = \"{}/{}\".format(s3path, new_engagement_basename)\n",
    "transfer.upload_file(new_engagement_basename, data_bucket, new_s3_name)\n",
    "\n",
    "# Update the \"main\" file\n",
    "transfer.upload_file(new_engagement_basename, data_bucket, engagement_key)\n",
    "\n",
    "# Update the dashboard file\n",
    "dash_bucket = \"net-mozaws-prod-metrics-data\"\n",
    "dash_s3_name = \"firefox-dashboard/{}\".format(engagement_basename)\n",
    "transfer.upload_file(new_engagement_basename, dash_bucket, dash_s3_name,\n",
    "                     extra_args={'ACL': 'bucket-owner-full-control'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
