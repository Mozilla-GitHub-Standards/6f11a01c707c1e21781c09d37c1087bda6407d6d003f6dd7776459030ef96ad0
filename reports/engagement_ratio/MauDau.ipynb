{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Firefox Engagement Ratio\n",
    "\n",
    "Compute the Engagement Ratio for the overall Firefox population as described in [Bug 1240849](https://bugzilla.mozilla.org/show_bug.cgi?id=1240849). The resulting data is shown on the [Firefox Dashboard](http://metrics.services.mozilla.com/firefox-dashboard/), and the more granular MAU and DAU values can be viewed via the [Diagnostic Data Viewer](https://metrics.services.mozilla.com/diagnostic-data-viewer).\n",
    "\n",
    "The actual Daily Active Users (DAU) and Monthly Active Users (MAU) computations are defined in [standards.py](https://github.com/mozilla/python_moztelemetry/blob/master/moztelemetry/standards.py) in the [python_moztelemetry](https://github.com/mozilla/python_moztelemetry) repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 4 ms, total: 8 ms\n",
      "Wall time: 21.9 s\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime as _datetime, timedelta, date\n",
    "import boto3\n",
    "import botocore\n",
    "import csv\n",
    "import os.path\n",
    "\n",
    "bucket = \"telemetry-parquet\"\n",
    "prefix = \"main_summary/v3\"\n",
    "%time dataset = sqlContext.read.load(\"s3://{}/{}\".format(bucket, prefix), \"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many cores are we running on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "640"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what do the underlying records look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- document_id: string (nullable = false)\n",
      " |-- client_id: string (nullable = true)\n",
      " |-- sample_id: integer (nullable = true)\n",
      " |-- channel: string (nullable = true)\n",
      " |-- normalized_channel: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- os: string (nullable = true)\n",
      " |-- os_version: string (nullable = true)\n",
      " |-- os_service_pack_major: string (nullable = true)\n",
      " |-- os_service_pack_minor: string (nullable = true)\n",
      " |-- profile_creation_date: integer (nullable = true)\n",
      " |-- subsession_start_date: string (nullable = true)\n",
      " |-- subsession_length: integer (nullable = true)\n",
      " |-- distribution_id: string (nullable = true)\n",
      " |-- submission_date: string (nullable = false)\n",
      " |-- sync_configured: boolean (nullable = true)\n",
      " |-- sync_count_desktop: integer (nullable = true)\n",
      " |-- sync_count_mobile: integer (nullable = true)\n",
      " |-- app_build_id: string (nullable = true)\n",
      " |-- app_display_version: string (nullable = true)\n",
      " |-- app_name: string (nullable = true)\n",
      " |-- app_version: string (nullable = true)\n",
      " |-- timestamp: long (nullable = false)\n",
      " |-- env_build_id: string (nullable = true)\n",
      " |-- env_build_version: string (nullable = true)\n",
      " |-- env_build_arch: string (nullable = true)\n",
      " |-- e10s_enabled: boolean (nullable = true)\n",
      " |-- e10s_cohort: string (nullable = true)\n",
      " |-- locale: string (nullable = true)\n",
      " |-- active_experiment_id: string (nullable = true)\n",
      " |-- active_experiment_branch: string (nullable = true)\n",
      " |-- reason: string (nullable = true)\n",
      " |-- timezone_offset: integer (nullable = true)\n",
      " |-- plugin_hangs: integer (nullable = true)\n",
      " |-- aborts_plugin: integer (nullable = true)\n",
      " |-- aborts_content: integer (nullable = true)\n",
      " |-- aborts_gmplugin: integer (nullable = true)\n",
      " |-- crashes_detected_plugin: integer (nullable = true)\n",
      " |-- crashes_detected_content: integer (nullable = true)\n",
      " |-- crashes_detected_gmplugin: integer (nullable = true)\n",
      " |-- crash_submit_attempt_main: integer (nullable = true)\n",
      " |-- crash_submit_attempt_content: integer (nullable = true)\n",
      " |-- crash_submit_attempt_plugin: integer (nullable = true)\n",
      " |-- crash_submit_success_main: integer (nullable = true)\n",
      " |-- crash_submit_success_content: integer (nullable = true)\n",
      " |-- crash_submit_success_plugin: integer (nullable = true)\n",
      " |-- active_addons_count: integer (nullable = true)\n",
      " |-- flash_version: string (nullable = true)\n",
      " |-- vendor: string (nullable = true)\n",
      " |-- is_default_browser: boolean (nullable = true)\n",
      " |-- default_search_engine_data_name: string (nullable = true)\n",
      " |-- loop_activity_open_panel: integer (nullable = true)\n",
      " |-- loop_activity_open_conversation: integer (nullable = true)\n",
      " |-- loop_activity_room_open: integer (nullable = true)\n",
      " |-- loop_activity_room_share: integer (nullable = true)\n",
      " |-- loop_activity_room_delete: integer (nullable = true)\n",
      " |-- devtools_toolbox_opened_count: integer (nullable = true)\n",
      " |-- search_counts: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = false)\n",
      " |    |    |-- engine: string (nullable = false)\n",
      " |    |    |-- source: string (nullable = false)\n",
      " |    |    |-- count: long (nullable = false)\n",
      " |-- submission_date_s3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to incrementally update the data, re-computing any values that are missing or for which data is still arriving. Define that logic here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to parse whitelist (/home/hadoop/anaconda2/lib/python2.7/site-packages/moztelemetry/histogram-whitelists.json). Assuming all histograms are acceptable.\n"
     ]
    }
   ],
   "source": [
    "def fmt(the_date, date_format=\"%Y%m%d\"):\n",
    "    return _datetime.strftime(the_date, date_format)\n",
    "\n",
    "# Our calculations look for activity date reported within\n",
    "# a certain time window. If that window has passed, we do\n",
    "# not need to re-compute data for that period.\n",
    "def should_be_updated(record,\n",
    "        target_col=\"day\",\n",
    "        generated_col=\"generated_on\",\n",
    "        date_format=\"%Y%m%d\"):\n",
    "    target = _datetime.strptime(record[target_col], date_format)\n",
    "    generated = _datetime.strptime(record[generated_col], date_format)\n",
    "    \n",
    "    # Don't regenerate data that was already updated today.\n",
    "    today = fmt(_datetime.utcnow(), date_format)\n",
    "    if record[generated_col] >= today:\n",
    "        return False\n",
    "    \n",
    "    diff = generated - target\n",
    "    return diff.days <= 10\n",
    "\n",
    "\n",
    "from moztelemetry.standards import filter_date_range, count_distinct_clientids\n",
    "\n",
    "# Similar to the version in standards.py, but uses subsession_start_date\n",
    "# instead of activityTimestamp\n",
    "def dau(dataframe, target_day, future_days=10, date_format=\"%Y%m%d\"):\n",
    "    \"\"\"Compute Daily Active Users (DAU) from the Executive Summary dataset.\n",
    "    See https://bugzilla.mozilla.org/show_bug.cgi?id=1240849\n",
    "    \"\"\"\n",
    "    target_day_date = _datetime.strptime(target_day, date_format)\n",
    "    min_activity = _datetime.strftime(target_day_date, \"%Y-%m-%d\")\n",
    "    max_activity = _datetime.strftime(target_day_date + timedelta(1), \"%Y-%m-%d\")\n",
    "    act_col = dataframe.subsession_start_date\n",
    "\n",
    "    min_submission = target_day\n",
    "    max_submission_date = target_day_date + timedelta(future_days)\n",
    "    max_submission = _datetime.strftime(max_submission_date, date_format)\n",
    "    sub_col = dataframe.submission_date_s3\n",
    "\n",
    "    filtered = filter_date_range(dataframe, act_col, min_activity, max_activity,\n",
    "        sub_col, min_submission, max_submission)\n",
    "    return count_distinct_clientids(filtered)\n",
    "\n",
    "# Similar to the version in standards.py, but uses subsession_start_date\n",
    "# instead of activityTimestamp\n",
    "def mau(dataframe, target_day, past_days=28, future_days=10, date_format=\"%Y%m%d\"):\n",
    "    \"\"\"Compute Monthly Active Users (MAU) from the Executive Summary dataset.\n",
    "    See https://bugzilla.mozilla.org/show_bug.cgi?id=1240849\n",
    "    \"\"\"\n",
    "    target_day_date = _datetime.strptime(target_day, date_format)\n",
    "\n",
    "    # Compute activity over `past_days` days leading up to target_day\n",
    "    min_activity_date = target_day_date - timedelta(past_days)\n",
    "    min_activity = _datetime.strftime(min_activity_date, \"%Y-%m-%d\")\n",
    "    max_activity = _datetime.strftime(target_day_date + timedelta(1), \"%Y-%m-%d\")\n",
    "    act_col = dataframe.subsession_start_date\n",
    "\n",
    "    min_submission = _datetime.strftime(min_activity_date, date_format)\n",
    "    max_submission_date = target_day_date + timedelta(future_days)\n",
    "    max_submission = _datetime.strftime(max_submission_date, date_format)\n",
    "    sub_col = dataframe.submission_date_s3\n",
    "\n",
    "    filtered = filter_date_range(dataframe, act_col, min_activity, max_activity,\n",
    "        sub_col, min_submission, max_submission)\n",
    "    return count_distinct_clientids(filtered)\n",
    "\n",
    "# Identify all missing days, or days that have not yet passed\n",
    "# the \"still reporting in\" threshold (as of 2016-03-17, that is\n",
    "# defined as 10 days).\n",
    "def update_engagement_csv(dataset, old_filename, new_filename, \n",
    "                          cutoff_days=30, date_format=\"%Y%m%d\"):\n",
    "    cutoff_date = _datetime.utcnow() - timedelta(cutoff_days)\n",
    "    cutoff = fmt(cutoff_date, date_format)\n",
    "    print \"Cutoff date: {}\".format(cutoff)\n",
    "\n",
    "    fields = [\"day\", \"dau\", \"mau\", \"generated_on\"]\n",
    "\n",
    "    should_write_header = True\n",
    "    potential_updates = {}\n",
    "    # Carry over rows we won't touch\n",
    "    if os.path.exists(old_filename):\n",
    "        with open(old_filename) as csv_old:\n",
    "            reader = csv.DictReader(csv_old)\n",
    "            with open(new_filename, \"w\") as csv_new:\n",
    "                writer = csv.DictWriter(csv_new, fields)\n",
    "                writer.writeheader()\n",
    "                should_write_header = False\n",
    "                for row in reader:\n",
    "                    if row['day'] < cutoff:\n",
    "                        writer.writerow(row)\n",
    "                    else:\n",
    "                        potential_updates[row['day']] = row\n",
    "\n",
    "    with open(new_filename, \"a\") as csv_new:\n",
    "        writer = csv.DictWriter(csv_new, fields)\n",
    "        if should_write_header:\n",
    "            writer.writeheader()\n",
    "\n",
    "        for i in range(cutoff_days, 0, -1):\n",
    "            target_day = fmt(_datetime.utcnow() - timedelta(i), date_format)\n",
    "            if target_day in potential_updates and not should_be_updated(potential_updates[target_day]):\n",
    "                # It's fine as-is.\n",
    "                writer.writerow(potential_updates[target_day])\n",
    "            else:\n",
    "                # Update it.\n",
    "                print \"We should update data for {}\".format(target_day)\n",
    "                record = {\"day\": target_day, \"generated_on\": fmt(_datetime.utcnow(), date_format)}\n",
    "                print \"Starting dau {} at {}\".format(target_day, _datetime.utcnow())\n",
    "                record[\"dau\"] = dau(dataset, target_day)\n",
    "                print \"Finished dau {} at {}\".format(target_day, _datetime.utcnow())\n",
    "                print \"Starting mau {} at {}\".format(target_day, _datetime.utcnow())\n",
    "                record[\"mau\"] = mau(dataset, target_day)\n",
    "                print \"Finished mau {} at {}\".format(target_day, _datetime.utcnow())\n",
    "                writer.writerow(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch existing data from S3\n",
    "Attempt to fetch an existing data file from S3. If found, update it incrementally. Otherwise, re-compute the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from boto3.s3.transfer import S3Transfer\n",
    "data_bucket = \"net-mozaws-prod-us-west-2-pipeline-analysis\"\n",
    "engagement_basename = \"engagement_ratio.csv\"\n",
    "new_engagement_basename = \"engagement_ratio.{}.csv\".format(_datetime.strftime(_datetime.utcnow(), \"%Y%m%d\"))\n",
    "s3path = \"mreid/maudau\"\n",
    "engagement_key = \"{}/{}\".format(s3path, engagement_basename)\n",
    "\n",
    "client = boto3.client('s3', 'us-west-2')\n",
    "transfer = S3Transfer(client)\n",
    "\n",
    "try:\n",
    "    transfer.download_file(data_bucket, engagement_key, engagement_basename)\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    # If the file wasn't there, that's ok. Otherwise, abort!\n",
    "    if e.response['Error']['Code'] != \"404\":\n",
    "        raise e\n",
    "    else:\n",
    "        print \"Did not find an existing file at '{}'\".format(engagement_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reorganize dataset\n",
    "dataset = dataset.select(dataset.client_id.alias('clientId'), 'subsession_start_date', 'submission_date_s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutoff date: 20160411\n"
     ]
    }
   ],
   "source": [
    "update_engagement_csv(dataset, engagement_basename, new_engagement_basename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update data on S3\n",
    "Now we have an updated dataset on the local filesystem.\n",
    "\n",
    "Since it is so tiny, we keep a date-stamped backup of each dataset in addition to the \"latest\" file.\n",
    "\n",
    "Upload the updated file back to S3, as well as relaying it to the S3 bucket that automatically relays to the dashboard server. This final upload appears in the [Firefox Dashboard data dir](http://metrics.services.mozilla.com/firefox-dashboard/data/) as [engagement_ratio.csv](http://metrics.services.mozilla.com/firefox-dashboard/data/engagement_ratio.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Upload the updated csv file to S3\n",
    "\n",
    "# Update the day-specific file:\n",
    "new_s3_name = \"{}/{}\".format(s3path, new_engagement_basename)\n",
    "transfer.upload_file(new_engagement_basename, data_bucket, new_s3_name)\n",
    "\n",
    "# Update the \"main\" file\n",
    "transfer.upload_file(new_engagement_basename, data_bucket, engagement_key)\n",
    "\n",
    "# Update the dashboard file\n",
    "dash_bucket = \"net-mozaws-prod-metrics-data\"\n",
    "dash_s3_name = \"firefox-dashboard/{}\".format(engagement_basename)\n",
    "transfer.upload_file(new_engagement_basename, dash_bucket, dash_s3_name,\n",
    "                     extra_args={'ACL': 'bucket-owner-full-control'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
